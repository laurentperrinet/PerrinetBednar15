{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# master simulations\n",
    "\n",
    "All simulations needed for the Scientific Report paper / supp material:\n",
    "1. taking a bunch of images from Serre & al containing or not an animal, plus the laboratory set,\n",
    "2. compute edges and\n",
    "3. then doing statistics on that\n",
    "4. to see if we can classify them.\n",
    "\n",
    "\n",
    "## simulations for the manuscript\n",
    "\n",
    "````\n",
    "    rm -fr mat/edges/ms_* mat/ms_* figures/edges/ms_* figures/ms_* figures/figure*\n",
    "    frioul_batch  -n \"14,15,16\"  -M 36 'python experiment_ms.py'\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%cd -q ../test/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting experiment_ms.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile experiment_ms.py\n",
    "#! /usr/bin/env python\n",
    "# -*- coding: utf8 -*-\n",
    "from __future__ import division, print_function\n",
    "\"\"\"\n",
    "\n",
    "paper:\n",
    "1. taking a bunch of images from Serre & al containing or not an animal, plus the laboratory set,\n",
    "2. compute edges and\n",
    "3. then doing statistics on that\n",
    "4. to see if we can classify them.\n",
    "\n",
    "rm -fr mat/edges/srep_* mat/srep_* figures/edges/srep_* figures/srep_* figures/edges/srep_*\n",
    "frioul_batch  -n \"14,15,16\"  -M 36 'python experiment_ms.py'\n",
    "frioul_batch  -n \"17,18,19\"  -M 36 'python experiment_ms.py'\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "__author__ = \"(c) Laurent Perrinet INT - CNRS\"\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\") # agg-backend, so we can create figures without x-server (no PDF, just PNG etc.)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "exp = 'srep'\n",
    "verb = False\n",
    "\n",
    "## TODO precision (accross images) as alpha value?\n",
    "from SparseEdges import EdgeFactory\n",
    "ef = EdgeFactory('default_param.py')\n",
    "ef.pe.verbose = 50\n",
    "\n",
    "features = ['first', 'first_rot', 'first_full', 'first_chevron']\n",
    "features = ['first', 'first_rot', 'full', 'chevron']\n",
    "\n",
    "#######################################################################\n",
    "#                 table 1 : classification results                    #\n",
    "#######################################################################\n",
    "list_db = [['serre07_distractors', 'serre07_targets'],\n",
    "           ['serre07_distractors', 'treeshrew'],\n",
    "           ['serre07_distractors', 'laboratory'],\n",
    "          ]\n",
    "if verb: print(15*'-~-',  ' DOING THE NUMBER CRUNCHING ', 15*'-~-')\n",
    "for feature in features:\n",
    "    if verb: print(5*'-~-',  'feature: ', feature, 5*'-~-')\n",
    "    for databases in list_db:\n",
    "        if verb: print(5*'-*-',  'databases: ', databases, 5*'-*-')\n",
    "        # without noise\n",
    "        ef.svm(exp, databases=databases, feature=feature)\n",
    "        # with noise\n",
    "        ef.svm(exp + '_noise', databases=databases, feature=feature, noise=ef.pe.noise)\n",
    "\n",
    "m, s =  np.zeros((4, 2, 2)), np.zeros((4, 2, 2))\n",
    "for i, feature in enumerate(features):\n",
    "    for j, databases in enumerate(list_db):\n",
    "        try:\n",
    "            category = '_SVM_' + databases[0] + '_' + databases[1]\n",
    "            m[i, j, 0] = np.load('cache_dir/' + exp + category + '_' + feature + '.npy').mean()*100\n",
    "            s[i, j, 0] = np.load('cache_dir/' + exp + category + '_' + feature + '.npy').std()*100\n",
    "            category = '_noise_SVM_' + databases[0] + '_' + databases[1]\n",
    "            m[i, j, 1] = np.load('cache_dir/' + exp + category + '_' + feature + '.npy').mean()*100\n",
    "            s[i, j, 1] = np.load('cache_dir/' + exp + category + '_' + feature + '.npy').std()*100\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "print(60*'-')\n",
    "print('table 1: classification results')\n",
    "print(60*'-')\n",
    "print('\\% Categories & FO & FO_rot & SO & CM ')\n",
    "print(60*'-')\n",
    "print('Non-animal vs Artificial             & ' + '%.0f' % m[0, 1, 0] + '\\pm %.0f' % s[0, 1, 0] + '\\% & ' + '%.0f' % m[1, 1, 0] + '\\pm %.0f' % s[1, 1, 0]  + '\\% & ' + '%.0f' % m[2, 1, 0] + '\\pm %.0f' % s[2, 1, 0]  + '\\% & ' + '%.0f' % m[3, 1, 0] + '\\pm %.0f' % s[3, 1, 0]  + '\\% \\\\\\\\')\n",
    "print('Non-animal vs Animal                 & ' + '%.0f' % m[0, 0, 0] + '\\pm %.0f' % s[0, 0, 0] + '\\% & ' + '%.0f' % m[1, 0, 0] + '\\pm %.0f' % s[1, 0, 0]  + '\\% & ' + '%.0f' % m[2, 0, 0] + '\\pm %.0f' % s[2, 0, 0] + '\\% & ' + '%.0f' % m[3, 0, 0] + '\\pm %.0f' % s[3, 0, 0]   + '\\% \\\\\\\\')\n",
    "print('Non-animal (noise) vs Animal (noise) & ' + '%.0f' % m[0, 0, 1] + '\\pm %.0f' % s[0, 0, 1] + '\\% & ' + '%.0f' % m[1, 0, 1] + '\\pm %.0f' % s[1, 0, 1]  + '\\% & ' + '%.0f' % m[2, 0, 1] + '\\pm %.0f' % s[2, 0, 1] + '\\% & ' + '%.0f' % m[3, 0, 1] + '\\pm %.0f' % s[3, 0, 1]   + '\\% \\\\\\\\')\n",
    "\n",
    "\n",
    "#######################################################################\n",
    "#                 table 2 : classification results                    #\n",
    "#######################################################################\n",
    "r = np.zeros((4,4))\n",
    "categories = ['_close', '_far', '_head', '_whole']\n",
    "for i, feature in enumerate(features):\n",
    "#, 'first_chevron', 'first_full']):\n",
    "    if verb: print(5*'-~-',  'feature: ', feature, 5*'-~-')\n",
    "    for j, category in enumerate(categories):\n",
    "        try:\n",
    "            if verb: print(5*'-*-',  'category: ', category, 5*'-*-')\n",
    "            ef.svm(exp, databases=['serre07_distractors' + category, 'serre07_targets' + category], feature=feature)\n",
    "            r[i, j] = np.load('cache_dir/' + exp + '_SVM_serre07_distractors' + category + '_serre07_targets' + category + '_' + feature + '.npy').mean()*100\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "print(60*'-')\n",
    "print('table 2: sub-categories')\n",
    "print(60*'-')\n",
    "print('\\% Category & Human & Serre07 & FO & FO_rot & SO & CM ')\n",
    "print(60*'-')\n",
    "print('Head  & 78\\% & 92\\% & ' + '%.1f' % r[0, 0] + '\\% & ' + '%.1f' % r[1, 0] + '\\% & ' + '%.1f' % r[2, 0] + '\\% & ' + '%.1f' % r[3, 0] + '\\% \\\\\\\\')\n",
    "print('Close & 82\\% & 90\\% & ' + '%.1f' % r[0, 1] + '\\% & ' + '%.1f' % r[1, 1] + '\\% & ' + '%.1f' % r[2, 1] + '\\% & ' + '%.1f' % r[3, 1] + '\\% \\\\\\\\')\n",
    "print('Whole & 71\\% & 79\\% & ' + '%.1f' % r[0, 2] + '\\% & ' + '%.1f' % r[1, 2] + '\\% & ' + '%.1f' % r[2, 2] + '\\% & ' + '%.1f' % r[3, 2] + '\\% \\\\\\\\')\n",
    "print('Far   & 52\\% & 68\\% & ' + '%.1f' % r[0, 3] + '\\% & ' + '%.1f' % r[1, 3] + '\\% & ' + '%.1f' % r[2, 3] + '\\% & ' + '%.1f' % r[3, 3] + '\\% \\\\\\\\')\n",
    "\n",
    "\n",
    "#######################################################################\n",
    "#                 table 3 : more databases                            #\n",
    "#######################################################################\n",
    "ef = EdgeFactory('default_param.py')\n",
    "ef.pe.verbose = 50\n",
    "ef.N_X = 244\n",
    "ef.N_Y = 244\n",
    "ef.init()\n",
    "\n",
    "list_db = [['holle_distractors', 'holle_targets'],\n",
    "           ['holle_distractors', 'laboratory'],\n",
    "          ]\n",
    "for feature in features:\n",
    "    if verb: print(5*'-~-',  'feature: ', feature, 5*'-~-')\n",
    "    for databases in list_db:\n",
    "        if verb: print(5*'-*-',  'databases: ', databases, 5*'-*-')\n",
    "        # without noise\n",
    "        ef.svm(exp, databases=databases, feature=feature)\n",
    "        # with noise\n",
    "        ef.svm(exp + '_noise', databases=databases, feature=feature, noise=ef.pe.noise)\n",
    "\n",
    "m, s =  np.zeros((4, 2, 2)), np.zeros((4, 2, 2))\n",
    "for i, feature in enumerate(features):\n",
    "    #for j, category in enumerate(['_SVM_serre07_distractors_serre07_targets', '_noise_SVM_serre07_distractors_serre07_targets', '_SVM_serre07_distractors_laboratory']):\n",
    "    for j, databases in enumerate(list_db):\n",
    "        try:\n",
    "            category = '_SVM_' + databases[0] + '_' + databases[1]\n",
    "            m[i, j, 0] = np.load('cache_dir/' + exp + category + '_' + feature + '.npy').mean()*100\n",
    "            s[i, j, 0] = np.load('cache_dir/' + exp + category + '_' + feature + '.npy').std()*100\n",
    "            category = '_noise_SVM_' + databases[0] + '_' + databases[1]\n",
    "            m[i, j, 1] = np.load('cache_dir/' + exp + category + '_' + feature + '.npy').mean()*100\n",
    "            s[i, j, 1] = np.load('cache_dir/' + exp + category + '_' + feature + '.npy').std()*100\n",
    "        except Exception as e:\n",
    "            import sys, traceback\n",
    "            tb = sys.exc_info()[2]#, sys.exc_traceback.print_stack()\n",
    "            print(e, traceback.print_tb(tb))\n",
    "print(60*'-')\n",
    "print('table 3: more classification results')\n",
    "print(60*'-')\n",
    "print('\\% Categories & FO & FO_rot & SO & CM ')\n",
    "print(60*'-')\n",
    "print('Non-animal vs Artificial             & ' + '%.0f' % m[0, 1, 0] + '\\pm %.0f' % s[0, 1, 0] + '\\% & ' + '%.0f' % m[1, 1, 0] + '\\pm %.0f' % s[1, 1, 0]  + '\\% & ' + '%.0f' % m[2, 1, 0] + '\\pm %.0f' % s[2, 1, 0]  + '\\% & ' + '%.0f' % m[3, 1, 0] + '\\pm %.0f' % s[3, 1, 0]  + '\\% \\\\\\\\')\n",
    "print('Non-animal vs Animal                 & ' + '%.0f' % m[0, 0, 0] + '\\pm %.0f' % s[0, 0, 0] + '\\% & ' + '%.0f' % m[1, 0, 0] + '\\pm %.0f' % s[1, 0, 0]  + '\\% & ' + '%.0f' % m[2, 0, 0] + '\\pm %.0f' % s[2, 0, 0] + '\\% & ' + '%.0f' % m[3, 0, 0] + '\\pm %.0f' % s[3, 0, 0]   + '\\% \\\\\\\\')\n",
    "print('Non-animal (noise) vs Animal (noise) & ' + '%.0f' % m[0, 0, 1] + '\\pm %.0f' % s[0, 0, 1] + '\\% & ' + '%.0f' % m[1, 0, 1] + '\\pm %.0f' % s[1, 0, 1]  + '\\% & ' + '%.0f' % m[2, 0, 1] + '\\pm %.0f' % s[2, 0, 1] + '\\% & ' + '%.0f' % m[3, 0, 1] + '\\pm %.0f' % s[3, 0, 1]   + '\\% \\\\\\\\')\n",
    "\n",
    "## TODO precision (accross images) as alpha value?\n",
    "print('~~~~ Exiting gracefully ~~~~')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 2 is out of bounds for axis 1 with size 2\n",
      "index 2 is out of bounds for axis 1 with size 2\n",
      "index 2 is out of bounds for axis 1 with size 2\n",
      "index 2 is out of bounds for axis 1 with size 2\n",
      "------------------------------------------------------------\n",
      "table 1: classification results\n",
      "------------------------------------------------------------\n",
      "\\% Categories & FO & FO_rot & SO & CM \n",
      "------------------------------------------------------------\n",
      "Non-animal vs Artificial             & 88\\pm 2\\% & 72\\pm 4\\% & 98\\pm 1\\% & 88\\pm 2\\% \\\\\n",
      "Non-animal vs Animal                 & 76\\pm 2\\% & 70\\pm 3\\% & 84\\pm 2\\% & 77\\pm 2\\% \\\\\n",
      "Non-animal (noise) vs Animal (noise) & 73\\pm 3\\% & 70\\pm 3\\% & 79\\pm 3\\% & 70\\pm 3\\% \\\\\n",
      "------------------------------------------------------------\n",
      "table 2: sub-categories\n",
      "------------------------------------------------------------\n",
      "\\% Category & Human & Serre07 & FO & FO_rot & SO & CM \n",
      "------------------------------------------------------------\n",
      "Head  & 78\\% & 92\\% & 77.5\\% & 69.3\\% & 87.0\\% & 71.5\\% \\\\\n",
      "Close & 82\\% & 90\\% & 74.1\\% & 71.3\\% & 77.5\\% & 77.3\\% \\\\\n",
      "Whole & 71\\% & 79\\% & 84.1\\% & 69.3\\% & 82.0\\% & 69.4\\% \\\\\n",
      "Far   & 52\\% & 68\\% & 72.8\\% & 66.2\\% & 83.3\\% & 71.1\\% \\\\\n",
      "------------------------------------------------------------\n",
      "table 3: more classification results\n",
      "------------------------------------------------------------\n",
      "\\% Categories & FO & FO_rot & SO & CM \n",
      "------------------------------------------------------------\n",
      "Non-animal vs Artificial             & 83\\pm 3\\% & 61\\pm 4\\% & 100\\pm 0\\% & 90\\pm 2\\% \\\\\n",
      "Non-animal vs Animal                 & 80\\pm 3\\% & 71\\pm 3\\% & 83\\pm 3\\% & 75\\pm 3\\% \\\\\n",
      "Non-animal (noise) vs Animal (noise) & 80\\pm 3\\% & 71\\pm 3\\% & 77\\pm 3\\% & 73\\pm 3\\% \\\\\n",
      "~~~~ Exiting gracefully ~~~~\n"
     ]
    }
   ],
   "source": [
    "%run experiment_ms.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## more simulations to test the effect of parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting experiment_animals.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile experiment_animals.py\n",
    "#/usr/bin/env python\n",
    "# -*- coding: utf8 -*-\n",
    "from __future__ import division, print_function\n",
    "\"\"\"\n",
    "\n",
    "Doing an optional stuff: taking a bunch of images from Kirchner & Thorpe or\n",
    "Serre et al., 2007, compute edges and then doing statistics on that to see if\n",
    "we can classify them.\n",
    "\n",
    "rm -fr mat/edges/classifier_*npy mat/classifier_*npy figures/edges/classifier_* figures/classifier_*\n",
    "! frioul_batch  -n \"14,15,16\"  -M 36 'python experiment_animals.py'\n",
    "! frioul_batch  -n \"11,12,13,14,15,16\"  -M 72 'python experiment_animals.py'\n",
    "\n",
    "\"\"\"\n",
    "__author__ = \"(c) Laurent Perrinet INT - CNRS\"\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\") # agg-backend, so we can create figures without x-server (no PDF, just PNG etc.)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "exp, paramfile = 'supp', 'default_param.py'\n",
    "\n",
    "## TODO precision (accross images) as alpha value?\n",
    "\n",
    "from SparseEdges import EdgeFactory\n",
    "\n",
    "features = ['first', 'first_rot', 'full', 'chevron']\n",
    "\n",
    "#! parameter scan\n",
    "#!---------------\n",
    "# generic experiments\n",
    "for feature in features:\n",
    "    ef = EdgeFactory(paramfile)\n",
    "    for ef.pe.N_r in np.arange(1, 10):\n",
    "        ef.svm(exp, feature=feature, opt_notSVM='_N_r_' + str(ef.pe.N_r))\n",
    "        \n",
    "    ef = EdgeFactory(paramfile)\n",
    "    for svm_KL_m in np.logspace(-.75, .75, 10, base=10)*ef.pe.svm_KL_m:\n",
    "        ef.pe.svm_KL_m = svm_KL_m\n",
    "        ef.svm(exp, feature=feature, opt_SVM='_precomputed_KL_m_' + str(ef.pe.svm_KL_m).replace('.', '_'), kernel='precomputed')\n",
    "    \n",
    "    ef = EdgeFactory(paramfile)\n",
    "    for KL_type in ['sKL', 'JSD', 'KL']:\n",
    "        ef.svm(exp, feature=feature, opt_SVM='_precomputed_KL_type_' + KL_type, KL_type=KL_type, kernel='precomputed')\n",
    "\n",
    "    for kernel in ['rbf', 'precomputed']:\n",
    "        for svm_test_size in np.linspace(.1, 1., 10, endpoint=False):\n",
    "            ef = EdgeFactory(paramfile)\n",
    "            ef.pe.svm_test_size = svm_test_size\n",
    "            ef.svm(exp, feature=feature, opt_SVM='_testing_' + kernel + '_test_size_' + str(ef.pe.svm_test_size).replace('.', '_'), kernel=kernel)\n",
    "\n",
    "    for svm_log in [True, False]:\n",
    "        ef = EdgeFactory(paramfile)\n",
    "        ef.pe.svm_log = svm_log\n",
    "        ef.svm(exp, feature=feature, opt_notSVM='_svm_log_' + str(ef.pe.svm_log))\n",
    "\n",
    "    for svm_norm in [True, False]:\n",
    "        for feature in ['first', 'first_rot', 'full', 'chevron']:\n",
    "            for databases in (['serre07_distractors', 'serre07_targets'], ['serre07_distractors', 'laboratory']): # ['serre07_targets', 'serre07_distractors']\n",
    "                ef = EdgeFactory(paramfile)\n",
    "                ef.pe.svm_norm = svm_norm\n",
    "                ef.svm(exp, feature=feature, databases=databases)\n",
    "                ef.svm(exp + '_noise', feature=feature, databases=databases, noise=ef.pe.noise)\n",
    "\n",
    "    for svm_tol in np.logspace(-9., 0., 9, base=10, endpoint=False):\n",
    "        ef = EdgeFactory(paramfile)\n",
    "        ef.pe.svm_tol = svm_tol\n",
    "        ef.svm(exp, feature=feature, opt_SVM='_svm_tol_' + str(ef.pe.svm_tol).replace('.', '_'))\n",
    "\n",
    "# experiments specific to the way we compute the histograms\n",
    "for feature in ['full', 'chevron', 'first_chevron', 'first_full']:\n",
    "#     for ef.pe.N_Dtheta in [1, 24, 48]:\n",
    "#         ef.svm(exp, feature=feature, opt_notSVM='_N_Dtheta_' + str(ef.pe.N_Dtheta))\n",
    "#     ef.pe.N_Dtheta = pe.N_Dtheta\n",
    "\n",
    "    for d_min in np.logspace(-1., 0., 9, base=10, endpoint=False)*ef.pe.d_max:\n",
    "        ef = EdgeFactory(paramfile)\n",
    "        ef.pe.d_min = d_min\n",
    "        ef.svm(exp, feature=feature, opt_notSVM='_d_min_' + str(ef.pe.d_min).replace('.', '_'))\n",
    "\n",
    "    for d_max in np.linspace(ef.pe.d_min*1.1, ef.pe.d_max*2, 10):\n",
    "        ef = EdgeFactory(paramfile)\n",
    "        ef.pe.d_max = d_max\n",
    "        ef.svm(exp, feature=feature, opt_notSVM='_d_max_' + str(ef.pe.d_max).replace('.', '_'))\n",
    "\n",
    "    for do_rank in [True, False]:\n",
    "        for scale_invariant in [True, False]:\n",
    "            for multiscale in [True, False]:\n",
    "                for weight_by_distance in [True, False]:\n",
    "                    ef = EdgeFactory(paramfile)\n",
    "                    ef.pe.do_rank = do_rank\n",
    "                    ef.pe.scale_invariant = scale_invariant\n",
    "                    ef.pe.multiscale = multiscale\n",
    "                    ef.pe.weight_by_distance = weight_by_distance\n",
    "                    ef.svm(exp,  feature=feature, opt_notSVM='_do_rank_' + str(ef.pe.do_rank) + '_multiscale_' + str(ef.pe.multiscale) + '_scale_invariant_' + str(ef.pe.do_rank) + '_weight_by_distance_' + str(ef.pe.weight_by_distance))\n",
    "\n",
    "#     for ef.pe.multiscale in [True, False]:\n",
    "#         ef.svm(exp, opt_SVM=opt_SVM, feature=feature, opt_notSVM='_multiscale_' + str(ef.pe.multiscale))\n",
    "#     ef.pe.multiscale = pe.multiscale\n",
    "\n",
    "#     for ef.pe.kappa_phase in np.logspace(-3, 0, 5)*np.pi:\n",
    "#         ef.svm(exp, opt_SVM=opt_SVM, feature=feature, opt_notSVM='_kappa_phase_' + str(ef.pe.kappa_phase).replace('.', '_'))\n",
    "#     ef.pe.kappa_phase = pe.kappa_phase\n",
    "# for ef.pe.kappa_phase in np.logspace(-2., 0., 5, base=10)*np.pi:\n",
    "#     ef.svm(exp, opt_SVM=opt_SVM, opt_notSVM='_kappa_phase_' + str(ef.pe.kappa_phase))\n",
    "# ef.pe.kappa_phase = pe.kappa_phase\n",
    "\n",
    "#! cross-validation\n",
    "#!-----------------\n",
    "# list_db = [\n",
    "#             ['serre07_distractors', 'serre07_targets'],\n",
    "#             ['Yelmo', 'holle_distractors'],\n",
    "#             ['treeshrew', 'holle_distractors'],\n",
    "#             ['holle_targets', 'treeshrew'],\n",
    "#             ['Yelmo', 'holle_targets'],\n",
    "#             ['Yelmo', 'treeshrew'],\n",
    "#             ['natural', 'holle_distractors'],\n",
    "#             ['natural', 'holle_targets'],\n",
    "#             ['laboratory', 'holle_distractors'],\n",
    "#             ['laboratory', 'holle_targets'],\n",
    "#             ['holle_distractors', 'holle_targets'],\n",
    "#             ['holle_distractors', 'serre07_targets'],\n",
    "#             ['serre07_distractors', 'holle_targets'],\n",
    "#             ['natural', 'laboratory'],\n",
    "#             ]\n",
    "# for databases in list_db:#\n",
    "#     ef.compare(exp, databases=databases)\n",
    "#     ef.svm(exp, databases=databases)\n",
    "#\n",
    "#for databases in ['Yelmo', 'treeshrew', 'holle_distractors', 'holle_targets', 'natural', 'laboratory']:\n",
    "#    ef.compare(exp, databases=[databases, databases])\n",
    "#    ef.svm(exp, databases=[databases, databases])\n",
    "\n",
    "# ef.svm(exp, databases=['holle_distractors', 'holle_targets', 'serre07_distractors', 'serre07_targets', 'natural', 'laboratory'])\n",
    "# ef.svm(exp, databases=['serre07_distractors', 'serre07_targets', 'natural', 'laboratory'])\n",
    "# ef.compare(exp, databases=['holle_distractors', 'holle_targets'])\n",
    "# ef.svm(exp, databases=['holle_distractors', 'holle_targets'])\n",
    "# ef.svm(exp, databases=['holle_distractors', 'holle_targets'], log=False)\n",
    "\n",
    "\n",
    "\n",
    "## TODO:  would be interesting to see how f1-score changes with number of edges used, i.e. whether it settles down to that particular pattern or just jumps around.\n",
    "\n",
    "\n",
    "## TODO precision (accross images) as alpha value?\n",
    "\n",
    "print('~~~~ Exiting gracefully ~~~~')\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "%run experiment_animals.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## more simulations to test the effect of filters' shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting experiment_testing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile experiment_testing.py\n",
    "#! /usr/bin/env python\n",
    "# -*- coding: utf8 -*-\n",
    "from __future__ import division, print_function\n",
    "\"\"\"\n",
    "\n",
    "Doing even more stuff: taking a bunch of images, computing edges and then doing\n",
    "statistics on that.\n",
    "\n",
    "rm -fr mat/edges/testing_* mat/testing_* figures/edges/testing_* figures/testing_*\n",
    "frioul_batch  -n \"14,15,16\"  -M 36 'python experiment_testing.py'\n",
    "\n",
    "\"\"\"\n",
    "__author__ = \"(c) Laurent Perrinet INT - CNRS\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "exp, paramfile = 'testing', 'default_param.py'\n",
    "from NeuroTools.parameters import ParameterSet\n",
    "pe = ParameterSet(paramfile)\n",
    "\n",
    "from SparseEdges import EdgeFactory\n",
    "\n",
    "def init_pe(pe):\n",
    "    ef = EdgeFactory(pe)\n",
    "    ef.pe.verbose = 5\n",
    "    ef.pe.N = 256\n",
    "    ef.pe.N_image = 80\n",
    "    ef.init()\n",
    "    return ef\n",
    "\n",
    "\n",
    "# TODO: here, we are more interested in the processing of the database, not the comparison - use the correct function\n",
    "# TODO : annotate the efficiency of different LogGabor bases (RMSE?)\n",
    "# TODO: make a circular mask to avoid border effects coming with whitening...\n",
    "\n",
    "#! comparing databases\n",
    "#!--------------------\n",
    "ef = init_pe(pe) # EF.EdgeFactory(edge=mp, pe=parameters)\n",
    "# ef.compare('testing_vanilla')\n",
    "ef.svm(exp + '_vanilla')\n",
    "\n",
    "# TODO : make an experiment showing that using scale does not bring much\n",
    "##! comparing representation parameters\n",
    "##!------------------------------------\n",
    "#! shorter log-gabor filters\n",
    "pe_ = ParameterSet('default_param.py')\n",
    "pe_.B_theta=pe.B_theta*2\n",
    "ef = init_pe(pe_)\n",
    "ef.svm(exp + '_short')\n",
    "\n",
    "#! longer log-gabor filters\n",
    "pe_= ParameterSet('default_param.py')\n",
    "pe_.B_theta=pe.B_theta/2\n",
    "ef = init_pe(pe_)\n",
    "ef.svm(exp + '_long')\n",
    "\n",
    "## other candidate parameters from class MatchingPursuit:\n",
    "##    n_levels = 5, n_theta = 16, B_sf_ratio = 3.\n",
    "pe_= ParameterSet('default_param.py')\n",
    "pe_.n_theta=pe.n_theta*2\n",
    "ef = init_pe(pe_)\n",
    "ef.svm(exp + '_moretheta')\n",
    "\n",
    "# is whitening important?\n",
    "pe_= ParameterSet('default_param.py')\n",
    "pe_.do_whitening=False\n",
    "ef = init_pe(pe_)\n",
    "ef.svm(exp + '_nowhite')\n",
    "\n",
    "#! softy MP with a lower alpha value\n",
    "pe_ = ParameterSet('default_param.py')\n",
    "pe_.N=pe.N*2\n",
    "pe_.MP_alpha = .25\n",
    "ef = init_pe(pe_)\n",
    "ef.svm(exp + '_smooth')\n",
    "\n",
    "#! hard MP with a full alpha value\n",
    "pe_ = ParameterSet('default_param.py')#\n",
    "pe_.N=pe.N/2\n",
    "pe_.MP_alpha = 1.\n",
    "ef = init_pe(pe_) # EF.EdgeFactory(edge=mp, pe=parameters)\n",
    "# ef.compare('testing_hard')\n",
    "ef.svm(exp + '_hard')\n",
    "\n",
    "##! comparing edge methods\n",
    "##!-----------------------\n",
    "## parameters from class EdgeFactory:\n",
    "pe_ = ParameterSet('default_param.py')#\n",
    "pe_.N = pe.N*4\n",
    "ef = init_pe(pe_)\n",
    "ef.svm(exp + '_moreN')\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "%run experiment_testing.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## running all scripts locally or remotely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "experiment_folder = 'PerrinetBednar15'\n",
    "\n",
    "cluster = False\n",
    "cluster = True\n",
    "\n",
    "if cluster:\n",
    "    try:\n",
    "        from INT_cluster import Frioul\n",
    "        k = Frioul(experiment_folder, N_jobs = 64)\n",
    "    except Exception:\n",
    "        cluster = False\n",
    "else:\n",
    "    def run_cmd(cmd, doit=True):\n",
    "        import subprocess\n",
    "        print ('⚡︎ Running ⚡︎ ', cmd)\n",
    "        if doit:\n",
    "            stdout = subprocess.check_output([cmd], shell=True)\n",
    "            return stdout.decode()#.splitlines()\n",
    "\n",
    "do_update = True\n",
    "do_update = False\n",
    "\n",
    "do_cleanup = False\n",
    "do_cleanup = True\n",
    "\n",
    "do_run = False\n",
    "do_run = True\n",
    "\n",
    "if cluster and do_cleanup:\n",
    "    for cmd in [\n",
    "        #\"rm -fr results cache_dir \",\n",
    "        #\"find . -name *sparselets* -exec rm -fr {} \\\\;\",\n",
    "        \"find . -name *lock* -exec rm -fr {} \\\\;\",\n",
    "        #\"touch frioul; rm frioul* \",\n",
    "        ]:\n",
    "        print(k.run_on_cluster(cmd))\n",
    "\n",
    "# preparing\n",
    "experiments = ['experiment_' + name +  '.py' for name in ['ms', 'animals', 'testing']]\n",
    "if do_run:\n",
    "    # RUNNING\n",
    "    k.run_cmd (\"rsync -av ../database ../test\")\n",
    "    if cluster: k.push_to_cluster()\n",
    "    for experiment in experiments:\n",
    "        if cluster:\n",
    "            fullcmd = 'ipython {experiment}'.format(experiment=experiment)\n",
    "            for cmd in [\n",
    "                \"frioul_batch  -M {N_jobs} '{cmd}' \".format(N_jobs=k.N_jobs, cmd=fullcmd), \n",
    "                \"frioul_list_jobs -v |grep job_array_id |uniq -c\",\n",
    "                        ]:\n",
    "                print(k.run_on_cluster(cmd))\n",
    "        else:\n",
    "            fullcmd = 'ipython3 {experiment}'.format(experiment=experiment)\n",
    "            run_cmd (fullcmd)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GETTING the data\n",
    "import time, os\n",
    "if cluster:\n",
    "    while True:    \n",
    "        print(k.pull_from_cluster())\n",
    "        print(k.run_on_cluster(\"tail -n 10 {}\".format(os.path.join(k.PATH, 'debug.log'))))\n",
    "        print(k.run_on_cluster(\"frioul_list_jobs -v |grep job_array_id |uniq -c\"))\n",
    "        locks = k.run_cmd (\"find . -name *lock -exec ls -l {} \\;\")\n",
    "        print(locks)\n",
    "        if len(locks) == 0: break\n",
    "        time.sleep(100)               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "receiving incremental file list\n",
      "deleting cache_dir/supp_SVM_serre07_distractors_serre07_targets_full_precomputed_KL_type_KL.npy_lock\n",
      "cache_dir/\n",
      "\n",
      "sent 46 bytes  received 110,173 bytes  44,087.60 bytes/sec\n",
      "total size is 5,617,192,498  speedup is 50,963.92\n"
     ]
    }
   ],
   "source": [
    "! rsync -av -u --delete --exclude .AppleDouble --exclude .git perrinet.l@frioul.int.univ-amu.fr:/hpc/invibe/perrinet.l/science/PerrinetBednar15/{results,cache_dir,debug.log} ../test/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## version control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!git s"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!git commit -am' core simulations : more testing'; git push"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
